# -*- coding: utf-8 -*-
"""
Streamlit app: Th·∫©m ƒë·ªãnh ph∆∞∆°ng √°n kinh doanh/ s·ª≠ d·ª•ng v·ªën (pasdv.docx)
"""
import io
import os
import re
import math
import json
import zipfile
import datetime as dt
from typing import Dict, Any, Tuple, Optional
import numpy as np
import pandas as pd
import streamlit as st
# Docx parsing
try:
    from docx import Document
    from docx.shared import Pt, RGBColor, Inches
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    from docx.enum.table import WD_TABLE_ALIGNMENT
except Exception:
    Document = None
# Gemini
try:
    import google.generativeai as genai
except Exception:
    genai = None
# Plotly cho bi·ªÉu ƒë·ªì
try:
    import plotly.graph_objects as go
    import plotly.express as px
except Exception:
    go = None
    px = None
st.set_page_config(page_title="PASDV - Th·∫©m ƒë·ªãnh ph∆∞∆°ng √°n", page_icon="üíº", layout="wide")
# ========================== Helpers ==========================
FIELD_DEFAULTS = {
    "ten_khach_hang": "",
    "cccd": "",
    "noi_cu_tru": "",
    "so_dien_thoai": "",
    "muc_dich_vay": "",
    "tong_nhu_cau_von": 0.0,
    "von_doi_ung": 0.0,
    "so_tien_vay": 0.0,
    "lai_suat_nam": 10.0,
    "thoi_gian_vay_thang": 12,
    "ky_han_tra": "Th√°ng",
    "thu_nhap_thang": 0.0,
    "gia_tri_tsdb": 0.0,
    "tong_no_hien_tai": 0.0,
    "loi_nhuan_rong_nam": 0.0,
    "tong_von_dau_tu": 0.0,
}
def vnd_to_float(s: str) -> float:
    """Chuy·ªÉn chu·ªói ti·ªÅn t·ªá VN v·ªÅ float (h·ªó tr·ª£ d·∫•u . ngƒÉn c√°ch, , th·∫≠p ph√¢n)."""
    if s is None:
        return 0.0
    s = str(s)
    if "," in s and "." in s:
        s = s.replace(".", "").replace(",", ".")
    elif "," in s and "." not in s:
        s = s.replace(".", "")
        s = s.replace(",", ".")
    else:
        s = s.replace(".", "")
    s = s.replace("ƒë", "").replace("VND", "").replace("vnƒë", "").replace("‚Ç´", "").replace(" ", "")
    s = re.sub(r"[^\d\.\-]", "", s)
    try:
        return float(s) if s else 0.0
    except Exception:
        return 0.0
def format_vnd(amount: float) -> str:
    """ƒê·ªãnh d·∫°ng ti·ªÅn VND: 1.234.567"""
    try:
        return f"{float(amount):,.0f}".replace(",", ".")
    except Exception:
        return "0"
def format_vnd_float(amount: float) -> str:
    """ƒê·ªãnh d·∫°ng s·ªë th·∫≠p ph√¢n ki·ªÉu VN: 1.234.567,89"""
    try:
        s = f"{float(amount):,.2f}"
        s = s.replace(",", "_").replace(".", ",").replace("_", ".")
        return s
    except Exception:
        return "0,00"
def percent_to_float(s: str) -> float:
    """Chuy·ªÉn ƒë·ªïi chu·ªói ph·∫ßn trƒÉm sang s·ªë float; ch·∫•p nh·∫≠n '8,5' ho·∫∑c '8.5'."""
    if s is None:
        return 0.0
    s = str(s).replace(",", ".")
    m = re.search(r"(\d+(?:\.\d+)?)", s)
    return float(m.group(1)) if m else 0.0
def vn_money_input(label: str, value: float, key: Optional[str] = None, help: Optional[str] = None) -> float:
    """√î nh·∫≠p ti·ªÅn t·ªá ki·ªÉu VN: hi·ªÉn th·ªã 1.234.567 v√† parse l·∫°i v·ªÅ float."""
    raw = st.text_input(label, value=format_vnd(value), key=key, help=help)
    return float(vnd_to_float(raw))
def vn_percent_input(label: str, value: float, key: Optional[str] = None, help: Optional[str] = None) -> float:
    """√î nh·∫≠p ph·∫ßn trƒÉm linh ho·∫°t: cho ph√©p nh·∫≠p '8,5' ho·∫∑c '8.5'."""
    shown = f"{float(value):.2f}".replace(".", ",")
    raw = st.text_input(label, value=shown, key=key, help=help)
    return percent_to_float(raw)
def extract_from_docx(file_bytes: bytes) -> Dict[str, Any]:
    """ƒê·ªçc .docx PASDV v√† tr√≠ch xu·∫•t th√¥ng tin theo c·∫•u tr√∫c th·ª±c t·∫ø."""
    data = FIELD_DEFAULTS.copy()
    if Document is None:
        return data
    bio = io.BytesIO(file_bytes)
    doc = Document(bio)
    full_text = "\n".join([p.text for p in doc.paragraphs])
    lines = [line.strip() for line in full_text.split('\n') if line.strip()]
    full_text = "\n".join(lines)
    # === 1. TH√îNG TIN KH√ÅCH H√ÄNG ===
    ten_pattern1 = r"(?:\d+\.\s*)?H·ªç\s+v√†\s+t√™n\s*[:Ôºö]\s*([A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√Ç·∫¶·∫§·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥][a-z√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫®·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµA-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√Ç·∫¶·∫§·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥\s]+)"
    m = re.search(ten_pattern1, full_text, flags=re.IGNORECASE)
    if m:
        data["ten_khach_hang"] = m.group(1).strip()
    else:
        ten_pattern2 = r"(?:√îng|B√†)\s*$$ (?:b√†|√¥ng) $$\s*[:Ôºö]\s*([A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√Ç·∫¶·∫§·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥][a-z√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫®·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµA-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√Ç·∫¶·∫§·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥\s]+)"
        m = re.search(ten_pattern2, full_text, flags=re.IGNORECASE)
        if m:
            data["ten_khach_hang"] = m.group(1).strip()
   
    cccd_pattern = r"(?:CMND|CCCD)(?:\/(?:CCCD|CMND))?(?:\/h·ªô\s*chi·∫øu)?\s*[:Ôºö]\s*(\d{9,12})"
    m = re.search(cccd_pattern, full_text, flags=re.IGNORECASE)
    if m:
        data["cccd"] = m.group(1).strip()
    noi_cu_tru_pattern = r"N∆°i\s*c∆∞\s*tr√∫\s*[:Ôºö]\s*([^\n]+?)(?=\n|S·ªë\s*ƒëi·ªán\s*tho·∫°i|$)"
    m = re.search(noi_cu_tru_pattern, full_text, flags=re.IGNORECASE | re.DOTALL)
    if m:
        data["noi_cu_tru"] = m.group(1).strip()
    sdt_pattern = r"S·ªë\s*ƒëi·ªán\s*tho·∫°i\s*[:Ôºö]\s*(0\d{9,10})"
    m = re.search(sdt_pattern, full_text, flags=re.IGNORECASE)
    if m:
        data["so_dien_thoai"] = m.group(1).strip()
    # === 2. PH∆Ø∆†NG √ÅN S·ª¨ D·ª§NG V·ªêN ===
    muc_dich_pattern1 = r"M·ª•c\s*ƒë√≠ch\s*vay\s*[:Ôºö]\s*([^\n]+)"
    m = re.search(muc_dich_pattern1, full_text, flags=re.IGNORECASE)
    if m:
        data["muc_dich_vay"] = m.group(1).strip()
    else:
        muc_dich_pattern2 = r"V·ªën\s*vay\s*Agribank.*?[:Ôºö].*?(?:Th·ª±c\s*hi·ªán|S·ª≠\s*d·ª•ng\s*v√†o)\s*([^\n]+)"
        m = re.search(muc_dich_pattern2, full_text, flags=re.IGNORECASE | re.DOTALL)
        if m:
            data["muc_dich_vay"] = m.group(1).strip()[:200]
    tnc_pattern = r"(?:T·ªïng\s*nhu\s*c·∫ßu\s*v·ªën|1\.\s*T·ªïng\s*nhu\s*c·∫ßu\s*v·ªën)\s*[:Ôºö]\s*([\d\.,]+)"
    m = re.search(tnc_pattern, full_text, flags=re.IGNORECASE)
    if m:
        data["tong_nhu_cau_von"] = vnd_to_float(m.group(1))
    von_du_pattern = r"V·ªën\s*ƒë·ªëi\s*·ª©ng\s*(?:tham\s*gia)?[^\d]*([\d\.,]+)\s*ƒë·ªìng"
    m = re.search(von_du_pattern, full_text, flags=re.IGNORECASE)
    if m:
        data["von_doi_ung"] = vnd_to_float(m.group(1))
    so_tien_vay_pattern = r"V·ªën\s*vay\s*Agribank\s*(?:s·ªë\s*ti·ªÅn)?[:\s]*([\d\.,]+)\s*ƒë·ªìng"
    m = re.search(so_tien_vay_pattern, full_text, flags=re.IGNORECASE)
    if m:
        data["so_tien_vay"] = vnd_to_float(m.group(1))
    thoi_han_pattern = r"Th·ªùi\s*h·∫°n\s*vay\s*[:Ôºö]\s*(\d+)\s*th√°ng"
    m = re.search(thoi_han_pattern, full_text, flags=re.IGNORECASE)
    if m:
        data["thoi_gian_vay_thang"] = int(m.group(1))
    lai_suat_pattern = r"L√£i\s*su·∫•t\s*[:Ôºö]\s*([\d\.,]+)\s*%"
    m = re.search(lai_suat_pattern, full_text, flags=re.IGNORECASE)
    if m:
        data["lai_suat_nam"] = percent_to_float(m.group(1))
    # === 3. NGU·ªíN TR·∫¢ N·ª¢ & THU NH·∫¨P ===
    thu_nhap_du_an_pattern = r"T·ª´\s*ngu·ªìn\s*thu\s*c·ªßa\s*d·ª±\s*√°n[^\d]*([\d\.,]+)\s*ƒë·ªìng\s*/\s*th√°ng"
    m = re.search(thu_nhap_du_an_pattern, full_text, flags=re.IGNORECASE)
    thu_nhap_du_an = 0.0
    if m:
        thu_nhap_du_an = vnd_to_float(m.group(1))
    thu_nhap_luong_pattern = r"Thu\s*nh·∫≠p\s*t·ª´\s*l∆∞∆°ng\s*[:Ôºö]\s*([\d\.,]+)\s*ƒë·ªìng\s*/\s*th√°ng"
    m = re.search(thu_nhap_luong_pattern, full_text, flags=re.IGNORECASE)
    thu_nhap_luong = 0.0
    if m:
        thu_nhap_luong = vnd_to_float(m.group(1))
    tong_thu_nhap_pattern = r"T·ªïng\s*thu\s*nh·∫≠p\s*(?:·ªïn\s*ƒë·ªãnh)?\s*(?:h√†ng\s*)?th√°ng\s*[:Ôºö]\s*([\d\.,]+)\s*ƒë·ªìng"
    m = re.search(tong_thu_nhap_pattern, full_text, flags=re.IGNORECASE)
    if m:
        data["thu_nhap_thang"] = vnd_to_float(m.group(1))
    else:
        data["thu_nhap_thang"] = thu_nhap_luong + thu_nhap_du_an
    # === 4. T√ÄI S·∫¢N B·∫¢O ƒê·∫¢M ===
    tsdb_pattern1 = r"T√†i\s*s·∫£n\s*1[^\n]*Gi√°\s*tr·ªã\s*[:Ôºö]\s*([\d\.,]+)\s*ƒë·ªìng"
    m = re.search(tsdb_pattern1, full_text, flags=re.IGNORECASE | re.DOTALL)
    if m:
        data["gia_tri_tsdb"] = vnd_to_float(m.group(1))
    else:
        tsdb_pattern2 = r"Gi√°\s*tr·ªã\s*nh√†\s*d·ª±\s*ki·∫øn\s*mua\s*[:Ôºö]\s*([\d\.,]+)\s*ƒë·ªìng"
        m = re.search(tsdb_pattern2, full_text, flags=re.IGNORECASE)
        if m:
            data["gia_tri_tsdb"] = vnd_to_float(m.group(1))
    # === 5. TH√îNG TIN B·ªî SUNG ===
    loi_nhuan_pattern = r"L·ª£i\s*nhu·∫≠n\s*(?:r√≤ng)?\s*(?:nƒÉm)?[^\d]*([\d\.,]+)\s*ƒë·ªìng"
    m = re.search(loi_nhuan_pattern, full_text, flags=re.IGNORECASE)
    if m:
        data["loi_nhuan_rong_nam"] = vnd_to_float(m.group(1))
    elif thu_nhap_du_an > 0:
        data["loi_nhuan_rong_nam"] = thu_nhap_du_an * 12
    if data["tong_nhu_cau_von"] == 0 and (data["von_doi_ung"] + data["so_tien_vay"] > 0):
        data["tong_nhu_cau_von"] = data["von_doi_ung"] + data["so_tien_vay"]
    if data["tong_von_dau_tu"] == 0:
        data["tong_von_dau_tu"] = data["tong_nhu_cau_von"]
    if data["gia_tri_tsdb"] == 0 and data["tong_nhu_cau_von"] > 0:
        data["gia_tri_tsdb"] = data["tong_nhu_cau_von"]
    return data
def annuity_payment(principal: float, annual_rate_pct: float, months: int) -> float:
    r = annual_rate_pct / 100.0 / 12.0
    if months <= 0:
        return 0.0
    if r == 0:
        return principal / months
    pmt = principal * r * (1 + r) ** months / ((1 + r) ** months - 1)
    return pmt
def build_amortization(principal: float, annual_rate_pct: float, months: int, start_date: Optional[dt.date] = None) -> pd.DataFrame:
    if start_date is None:
        start_date = dt.date.today()
    r = annual_rate_pct / 100.0 / 12.0
    pmt = annuity_payment(principal, annual_rate_pct, months)
    schedule = []
    balance = principal
    for i in range(1, months + 1):
        interest = balance * r
        principal_pay = pmt - interest
        balance = max(0.0, balance - principal_pay)
        pay_date = start_date + dt.timedelta(days=30 * i)
        schedule.append({
            "K·ª≥": i,
            "Ng√†y thanh to√°n": pay_date.strftime("%d/%m/%Y"),
            "Ti·ªÅn l√£i": round(interest, 0),
            "Ti·ªÅn g·ªëc": round(principal_pay, 0),
            "T·ªïng ph·∫£i tr·∫£": round(pmt, 0),
            "D∆∞ n·ª£ c√≤n l·∫°i": round(balance, 0),
        })
    df = pd.DataFrame(schedule)
    return df
def style_schedule_table(df: pd.DataFrame) -> pd.DataFrame:
    """T√¥ m√†u b·∫£ng k·∫ø ho·∫°ch tr·∫£ n·ª£"""
    def color_row(row):
        if row['K·ª≥'] % 2 == 0:
            return ['background-color: #f0f8ff'] * len(row)
        else:
            return ['background-color: #ffffff'] * len(row)
    styled = df.style.apply(color_row, axis=1)
    styled = styled.format({
        'Ti·ªÅn l√£i': lambda x: format_vnd(x),
        'Ti·ªÅn g·ªëc': lambda x: format_vnd(x),
        'T·ªïng ph·∫£i tr·∫£': lambda x: format_vnd(x),
        'D∆∞ n·ª£ c√≤n l·∫°i': lambda x: format_vnd(x)
    })
    styled = styled.set_properties(**{
        'text-align': 'right',
        'font-size': '14px'
    }, subset=['Ti·ªÅn l√£i', 'Ti·ªÅn g·ªëc', 'T·ªïng ph·∫£i tr·∫£', 'D∆∞ n·ª£ c√≤n l·∫°i'])
    styled = styled.set_properties(**{
        'text-align': 'center'
    }, subset=['K·ª≥', 'Ng√†y thanh to√°n'])
    return styled
def compute_metrics(d: Dict[str, Any]) -> Dict[str, Any]:
    res = {}
    pmt = annuity_payment(d.get("so_tien_vay", 0.0), d.get("lai_suat_nam", 0.0), d.get("thoi_gian_vay_thang", 0))
    thu_nhap_thang = max(1e-9, d.get("thu_nhap_thang", 0.0))
    res["PMT_thang"] = pmt
    res["DSR"] = pmt / thu_nhap_thang if thu_nhap_thang > 0 else np.nan
    tong_nhu_cau = d.get("tong_nhu_cau_von", 0.0)
    von_doi_ung = d.get("von_doi_ung", 0.0)
    so_tien_vay = d.get("so_tien_vay", 0.0)
    gia_tri_tsdb = d.get("gia_tri_tsdb", 0.0)
    tong_no_hien_tai = d.get("tong_no_hien_tai", 0.0)
    loi_nhuan_rong_nam = d.get("loi_nhuan_rong_nam", 0.0)
    tong_von_dau_tu = d.get("tong_von_dau_tu", 0.0)
    res["E_over_C"] = (von_doi_ung / tong_nhu_cau) if tong_nhu_cau > 0 else np.nan
    res["LTV"] = (so_tien_vay / gia_tri_tsdb) if gia_tri_tsdb > 0 else np.nan
    thu_nhap_nam = thu_nhap_thang * 12.0
    res["Debt_over_Income"] = (tong_no_hien_tai + so_tien_vay) / max(1e-9, thu_nhap_nam)
    res["ROI"] = (loi_nhuan_rong_nam / max(1e-9, tong_von_dau_tu)) if tong_von_dau_tu > 0 else np.nan
    res["CFR"] = ((thu_nhap_thang - pmt) / thu_nhap_thang) if thu_nhap_thang > 0 else np.nan
    res["Coverage"] = (gia_tri_tsdb / max(1e-9, so_tien_vay)) if so_tien_vay > 0 else np.nan
    res["Phuong_an_hop_ly"] = math.isclose(tong_nhu_cau, von_doi_ung + so_tien_vay, rel_tol=0.02, abs_tol=1_000_000)
    score = 0.0
    if not np.isnan(res["DSR"]):
        score += max(0.0, 1.0 - min(1.0, res["DSR"])) * 0.25
    if not np.isnan(res["LTV"]):
        score += max(0.0, 1.0 - min(1.0, res["LTV"])) * 0.25
    if not np.isnan(res["E_over_C"]):
        score += min(1.0, res["E_over_C"] / 0.3) * 0.2
    if not np.isnan(res["CFR"]):
        score += max(0.0, min(1.0, (res["CFR"]))) * 0.2
    if not np.isnan(res["Coverage"]):
        score += min(1.0, (res["Coverage"] / 1.5)) * 0.1
    res["Score_AI_demo"] = round(score, 3)
    return res
def create_metrics_chart(metrics: Dict[str, Any]):
    """T·∫°o bi·ªÉu ƒë·ªì tr·ª±c quan cho c√°c ch·ªâ ti√™u t√†i ch√≠nh ch√≠nh"""
    if go is None or px is None:
        st.warning("Th∆∞ vi·ªán Plotly ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Kh√¥ng th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì.")
        return
    df_metrics = pd.DataFrame({
        "Ch·ªâ ti√™u": ["DSR", "LTV", "E/C", "Coverage", "CFR"],
        "Gi√° tr·ªã": [
            metrics.get("DSR", np.nan),
            metrics.get("LTV", np.nan),
            metrics.get("E_over_C", np.nan),
            metrics.get("Coverage", np.nan),
            metrics.get("CFR", np.nan),
        ],
        "Ng∆∞·ª°ng tham chi·∫øu": [0.8, 0.8, 0.2, 1.2, 0.0]
    })
    df_metrics = df_metrics.dropna(subset=['Gi√° tr·ªã']).reset_index(drop=True)
    if df_metrics.empty:
        st.info("Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì ch·ªâ ti√™u t√†i ch√≠nh.")
        return
    def get_color(row):
        metric = row['Ch·ªâ ti√™u']
        value = row['Gi√° tr·ªã']
        ref = row['Ng∆∞·ª°ng tham chi·∫øu']
        if metric in ["DSR", "LTV"]:
            return "green" if value <= ref else "red"
        elif metric in ["E/C", "Coverage", "CFR"]:
            return "green" if value >= ref else "red"
        return "gray"
    df_metrics['M√†u'] = df_metrics.apply(get_color, axis=1)
    df_metrics['Gi√° tr·ªã (%)'] = df_metrics['Gi√° tr·ªã'] * 100
    fig = px.bar(
        df_metrics,
        x="Ch·ªâ ti√™u",
        y="Gi√° tr·ªã (%)",
        color="M√†u",
        color_discrete_map={"green": "#28a745", "red": "#dc3545", "gray": "#6c757d"},
        text=df_metrics['Gi√° tr·ªã (%)'].apply(lambda x: f"{x:,.1f}%"),
        title="Bi·ªÉu ƒë·ªì Ch·ªâ ti√™u T√†i ch√≠nh (CADAP)",
        labels={"Gi√° tr·ªã (%)": "Gi√° tr·ªã (%)", "Ch·ªâ ti√™u": "Ch·ªâ ti√™u"},
    )
    for index, row in df_metrics.iterrows():
        metric = row['Ch·ªâ ti√™u']
        ref_value = row['Ng∆∞·ª°ng tham chi·∫øu'] * 100
        color = "#ffc107" if ref_value > 0 else "#007bff"
        if metric in ["DSR", "LTV"]:
            fig.add_shape(
                type="line",
                x0=index - 0.4, x1=index + 0.4, y0=ref_value, y1=ref_value,
                line=dict(color=color, width=2, dash="dash"),
                xref="x", yref="y",
                name=f"Ng∆∞·ª°ng {metric}"
            )
            fig.add_annotation(
                x=index, y=ref_value + 3,
                text=f"Max {ref_value:g}%", showarrow=False,
                font=dict(color=color, size=10),
            )
        elif metric in ["E/C", "Coverage"]:
            fig.add_shape(
                type="line",
                x0=index - 0.4, x1=index + 0.4, y0=ref_value, y1=ref_value,
                line=dict(color=color, width=2, dash="dash"),
                xref="x", yref="y",
                name=f"Ng∆∞·ª°ng {metric}"
            )
            fig.add_annotation(
                x=index, y=ref_value - 3,
                text=f"Min {ref_value:g}%", showarrow=False,
                font=dict(color=color, size=10),
            )
    fig.update_layout(
        showlegend=False,
        yaxis_title="Gi√° tr·ªã (%)",
        xaxis_title="Ch·ªâ ti√™u",
        hovermode="x unified"
    )
    st.plotly_chart(fig, use_container_width=True)
def gemini_analyze(d: Dict[str, Any], metrics: Dict[str, Any], model_name: str, api_key: str) -> str:
    if genai is None:
        return "Th∆∞ vi·ªán google-generativeai ch∆∞a ƒë∆∞·ª£c c√†i. Vui l√≤ng th√™m 'google-generativeai' v√†o requirements.txt."
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name)
        d_formatted = {k: format_vnd(v) if isinstance(v, (int, float)) and k != 'lai_suat_nam' else v for k, v in d.items()}
        metrics_formatted = {
            k: (f"{v*100:,.1f}%"
                if k not in ["PMT_thang", "Debt_over_Income", "Score_AI_demo"] and not np.isnan(v)
                else format_vnd(v) if k == "PMT_thang"
                else f"{v:,.2f}")
            for k, v in metrics.items()
        }
        prompt = f"""
B·∫°n l√† chuy√™n vi√™n t√≠n d·ª•ng. Ph√¢n t√≠ch h·ªì s∆° vay sau (JSON) v√† ƒë∆∞a ra ƒë·ªÅ xu·∫•t "Cho vay" / "Cho vay c√≥ ƒëi·ªÅu ki·ªán" / "Kh√¥ng cho vay" k√®m gi·∫£i th√≠ch ng·∫Øn g·ªçn (<=200 t·ª´).
JSON ƒë·∫ßu v√†o:
Kh√°ch h√†ng & ph∆∞∆°ng √°n: {json.dumps(d_formatted, ensure_ascii=False)}
Ch·ªâ ti√™u t√≠nh to√°n: {json.dumps(metrics_formatted, ensure_ascii=False)}
Ng∆∞·ª°ng tham chi·∫øu:
- DSR ‚â§ 0.8; LTV ‚â§ 0.8; E/C ‚â• 0.2; CFR > 0; Coverage > 1.2.
- N·∫øu th√¥ng tin thi·∫øu, h√£y n√™u gi·∫£ ƒë·ªãnh r√µ r√†ng.
"""
        resp = model.generate_content(prompt)
        return resp.text or "(Kh√¥ng c√≥ n·ªôi dung t·ª´ Gemini)"
    except Exception as e:
        return f"L·ªói khi g·ªçi Gemini: {e}"
def make_zip_for_download() -> bytes:
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as z:
        for fname in ["app.py", "requirements.txt", "README.md"]:
            if os.path.exists(fname):
                z.write(fname, arcname=fname)
    buf.seek(0)
    return buf.read()
def export_to_docx(data: Dict[str, Any], metrics: Dict[str, Any], schedule_df: pd.DataFrame, analysis: str = "") -> bytes:
    """Xu·∫•t b√°o c√°o th·∫©m ƒë·ªãnh ra file DOCX v·ªõi ƒë·ªãnh d·∫°ng ƒë·∫πp m·∫Øt"""
    if Document is None:
        return b""
    
    doc = Document()
    
    # Thi·∫øt l·∫≠p font m·∫∑c ƒë·ªãnh
    from docx.oxml.ns import qn
    doc.styles['Normal'].font.name = 'Times New Roman'
    doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'Times New Roman')
    doc.styles['Normal'].font.size = Pt(12)
    
    # Trang b√¨a
    doc.add_paragraph("NG√ÇN H√ÄNG N√îNG NGHI·ªÜP V√Ä PH√ÅT TRI·ªÇN N√îNG TH√îN VI·ªÜT NAM", style='Title').alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph("B√ÅO C√ÅO TH·∫®M ƒê·ªäNH PH∆Ø∆†NG √ÅN S·ª¨ D·ª§NG V·ªêN", style='Title').alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph(f"Ng√†y b√°o c√°o: {dt.date.today().strftime('%d/%m/%Y')}", style='Subtitle').alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph()
    doc.add_page_break()
    
    # PH·∫¶N I: TH√îNG TIN KH√ÅCH H√ÄNG
    doc.add_heading('I. TH√îNG TIN KH√ÅCH H√ÄNG', 1)
    table1 = doc.add_table(rows=5, cols=2)
    table1.style = 'Table Grid'
    table1.alignment = WD_TABLE_ALIGNMENT.CENTER
    table1.autofit = True
    for row in table1.rows:
        for cell in row.cells:
            cell.paragraphs[0].style.font.size = Pt(11)
            cell.paragraphs[0].style.font.name = 'Times New Roman'
    
    cells = table1.rows[0].cells
    cells[0].text = 'H·ªç v√† t√™n:'
    cells[1].text = data.get('ten_khach_hang', '')
    
    cells = table1.rows[1].cells
    cells[0].text = 'CMND/CCCD:'
    cells[1].text = data.get('cccd', '')
    
    cells = table1.rows[2].cells
    cells[0].text = 'N∆°i c∆∞ tr√∫:'
    cells[1].text = data.get('noi_cu_tru', '')
    
    cells = table1.rows[3].cells
    cells[0].text = 'S·ªë ƒëi·ªán tho·∫°i:'
    cells[1].text = data.get('so_dien_thoai', '')
    
    cells = table1.rows[4].cells
    cells[0].text = 'M·ª•c ƒë√≠ch vay:'
    cells[1].text = data.get('muc_dich_vay', '')
    
    doc.add_paragraph()
    
    # PH·∫¶N II: TH√îNG TIN KHO·∫¢N VAY
    doc.add_heading('II. TH√îNG TIN KHO·∫¢N VAY', 1)
    table2 = doc.add_table(rows=7, cols=2)
    table2.style = 'Table Grid'
    table2.alignment = WD_TABLE_ALIGNMENT.CENTER
    table2.autofit = True
    for row in table2.rows:
        for cell in row.cells:
            cell.paragraphs[0].style.font.size = Pt(11)
            cell.paragraphs[0].style.font.name = 'Times New Roman'
    
    cells = table2.rows[0].cells
    cells[0].text = 'T·ªïng nhu c·∫ßu v·ªën:'
    cells[1].text = f"{format_vnd(data.get('tong_nhu_cau_von', 0))} VND"
    
    cells = table2.rows[1].cells
    cells[0].text = 'V·ªën ƒë·ªëi ·ª©ng:'
    cells[1].text = f"{format_vnd(data.get('von_doi_ung', 0))} VND"
    
    cells = table2.rows[2].cells
    cells[0].text = 'S·ªë ti·ªÅn vay:'
    cells[1].text = f"{format_vnd(data.get('so_tien_vay', 0))} VND"
    
    cells = table2.rows[3].cells
    cells[0].text = 'L√£i su·∫•t:'
    cells[1].text = f"{data.get('lai_suat_nam', 0):.2f}%/nƒÉm"
    
    cells = table2.rows[4].cells
    cells[0].text = 'Th·ªùi h·∫°n vay:'
    cells[1].text = f"{data.get('thoi_gian_vay_thang', 0)} th√°ng"
    
    cells = table2.rows[5].cells
    cells[0].text = 'Thu nh·∫≠p th√°ng:'
    cells[1].text = f"{format_vnd(data.get('thu_nhap_thang', 0))} VND"
    
    cells = table2.rows[6].cells
    cells[0].text = 'Gi√° tr·ªã TSƒêB:'
    cells[1].text = f"{format_vnd(data.get('gia_tri_tsdb', 0))} VND"
    
    doc.add_paragraph()
    
    # PH·∫¶N III: CH·ªà TI√äU T√ÄI CH√çNH
    doc.add_heading('III. CH·ªà TI√äU T√ÄI CH√çNH (CADAP)', 1)
    table3 = doc.add_table(rows=8, cols=3)
    table3.style = 'Table Grid'
    table3.alignment = WD_TABLE_ALIGNMENT.CENTER
    table3.autofit = True
    for i, row in enumerate(table3.rows):
        for cell in row.cells:
            cell.paragraphs[0].style.font.size = Pt(11)
            cell.paragraphs[0].style.font.name = 'Times New Roman'
            if i == 0:  # Header row
                cell.paragraphs[0].style.font.bold = True
                cell.paragraphs[0].style.font.color.rgb = RGBColor(0, 0, 0)
            elif i % 2 == 0:
                cell.paragraphs[0].style.font.color.rgb = RGBColor(0, 0, 139)
    
    hdr_cells = table3.rows[0].cells
    hdr_cells[0].text = 'Ch·ªâ ti√™u'
    hdr_cells[1].text = 'Gi√° tr·ªã'
    hdr_cells[2].text = 'ƒê√°nh gi√°'
    
    cells = table3.rows[1].cells
    cells[0].text = 'PMT (Ti·ªÅn tr·∫£/th√°ng)'
    cells[1].text = f"{format_vnd(metrics.get('PMT_thang', 0))} VND"
    cells[2].text = ''
    
    cells = table3.rows[2].cells
    cells[0].text = 'DSR (Debt Service Ratio)'
    dsr = metrics.get('DSR', 0)
    cells[1].text = f"{dsr*100:.1f}%" if not np.isnan(dsr) else 'n/a'
    cells[2].text = '‚úì ƒê·∫°t' if (not np.isnan(dsr) and dsr <= 0.8) else '‚úó Kh√¥ng ƒë·∫°t'
    
    cells = table3.rows[3].cells
    cells[0].text = 'LTV (Loan to Value)'
    ltv = metrics.get('LTV', 0)
    cells[1].text = f"{ltv*100:.1f}%" if not np.isnan(ltv) else 'n/a'
    cells[2].text = '‚úì ƒê·∫°t' if (not np.isnan(ltv) and ltv <= 0.8) else '‚úó Kh√¥ng ƒë·∫°t'
    
    cells = table3.rows[4].cells
    cells[0].text = 'E/C (Equity to Capital)'
    ec = metrics.get('E_over_C', 0)
    cells[1].text = f"{ec*100:.1f}%" if not np.isnan(ec) else 'n/a'
    cells[2].text = '‚úì ƒê·∫°t' if (not np.isnan(ec) and ec >= 0.2) else '‚úó Kh√¥ng ƒë·∫°t'
    
    cells = table3.rows[5].cells
    cells[0].text = 'CFR (Cash Flow Ratio)'
    cfr = metrics.get('CFR', 0)
    cells[1].text = f"{cfr*100:.1f}%" if not np.isnan(cfr) else 'n/a'
    cells[2].text = '‚úì ƒê·∫°t' if (not np.isnan(cfr) and cfr > 0) else '‚úó Kh√¥ng ƒë·∫°t'
    
    cells = table3.rows[6].cells
    cells[0].text = 'Coverage (Collateral Coverage)'
    cov = metrics.get('Coverage', 0)
    cells[1].text = f"{cov*100:.1f}%" if not np.isnan(cov) else 'n/a'
    cells[2].text = '‚úì ƒê·∫°t' if (not np.isnan(cov) and cov > 1.2) else '‚úó Kh√¥ng ƒë·∫°t'
    
    cells = table3.rows[7].cells
    cells[0].text = 'Score t·ªïng h·ª£p'
    cells[1].text = f"{metrics.get('Score_AI_demo', 0):.3f}"
    score = metrics.get('Score_AI_demo', 0)
    cells[2].text = '‚úì T·ªët' if score >= 0.7 else ('‚ö† Trung b√¨nh' if score >= 0.5 else '‚úó Y·∫øu')
    
    doc.add_paragraph()
    
    # PH·∫¶N IV: K·∫æ HO·∫†CH TR·∫¢ N·ª¢ (5 k·ª≥ ƒë·∫ßu)
    doc.add_heading('IV. K·∫æ HO·∫†CH TR·∫¢ N·ª¢ (5 k·ª≥ ƒë·∫ßu)', 1)
    n_rows = min(6, len(schedule_df) + 1)
    table4 = doc.add_table(rows=n_rows, cols=6)
    table4.style = 'Table Grid'
    table4.alignment = WD_TABLE_ALIGNMENT.CENTER
    table4.autofit = True
    for i, row in enumerate(table4.rows):
        for cell in row.cells:
            cell.paragraphs[0].style.font.size = Pt(11)
            cell.paragraphs[0].style.font.name = 'Times New Roman'
            if i == 0:  # Header row
                cell.paragraphs[0].style.font.bold = True
            elif i % 2 == 0:
                cell.paragraphs[0].style.font.color.rgb = RGBColor(0, 0, 139)
    
    hdr_cells = table4.rows[0].cells
    hdr_cells[0].text = 'K·ª≥'
    hdr_cells[1].text = 'Ng√†y'
    hdr_cells[2].text = 'Ti·ªÅn l√£i'
    hdr_cells[3].text = 'Ti·ªÅn g·ªëc'
    hdr_cells[4].text = 'T·ªïng tr·∫£'
    hdr_cells[5].text = 'D∆∞ n·ª£'
    
    for i in range(min(5, len(schedule_df))):
        row = schedule_df.iloc[i]
        cells = table4.rows[i+1].cells
        cells[0].text = str(row['K·ª≥'])
        cells[1].text = row['Ng√†y thanh to√°n']
        cells[2].text = format_vnd(row['Ti·ªÅn l√£i'])
        cells[3].text = format_vnd(row['Ti·ªÅn g·ªëc'])
        cells[4].text = format_vnd(row['T·ªïng ph·∫£i tr·∫£'])
        cells[5].text = format_vnd(row['D∆∞ n·ª£ c√≤n l·∫°i'])
    
    doc.add_paragraph()
    p = doc.add_paragraph(f"(Xem file Excel ƒë√≠nh k√®m ƒë·ªÉ c√≥ ƒë·∫ßy ƒë·ªß {len(schedule_df)} k·ª≥ thanh to√°n)")
    p.alignment = WD_ALIGN_PARAGRAPH.CENTER
    p.style.font.size = Pt(10)
    
    doc.add_paragraph()
    
    # PH·∫¶N V: PH√ÇN T√çCH V√Ä K·∫æT LU·∫¨N
    if analysis:
        doc.add_heading('V. PH√ÇN T√çCH V√Ä K·∫æT LU·∫¨N (AI)', 1)
        p = doc.add_paragraph(analysis)
        p.style.font.size = Pt(11)
        doc.add_paragraph()
    
    # PH·∫¶N VI: √ù KI·∫æN TH·∫®M ƒê·ªäNH
    doc.add_heading('VI. √ù KI·∫æN TH·∫®M ƒê·ªäNH', 1)
    score = metrics.get('Score_AI_demo', 0)
    dsr = metrics.get('DSR', 0)
    ltv = metrics.get('LTV', 0)
    
    if score >= 0.7 and (np.isnan(dsr) or dsr <= 0.8) and (np.isnan(ltv) or ltv <= 0.8):
        de_xuat = "‚òë ƒê·ªÄ XU·∫§T CHO VAY"
        ly_do = "H·ªì s∆° ƒë√°p ·ª©ng c√°c ch·ªâ ti√™u t√†i ch√≠nh, kh·∫£ nƒÉng tr·∫£ n·ª£ t·ªët, t√†i s·∫£n b·∫£o ƒë·∫£m ƒë·∫ßy ƒë·ªß."
    elif score >= 0.5:
        de_xuat = "‚òë ƒê·ªÄ XU·∫§T CHO VAY C√ì ƒêI·ªÄU KI·ªÜN"
        ly_do = "H·ªì s∆° c·∫ßn b·ªï sung th√™m t√†i s·∫£n b·∫£o ƒë·∫£m ho·∫∑c ƒëi·ªÅu ch·ªânh ƒëi·ªÅu ki·ªán vay ƒë·ªÉ gi·∫£m r·ªßi ro."
    else:
        de_xuat = "‚òê KH√îNG ƒê·ªÄ XU·∫§T CHO VAY"
        ly_do = "H·ªì s∆° kh√¥ng ƒë·∫°t c√°c ch·ªâ ti√™u t√†i ch√≠nh t·ªëi thi·ªÉu, r·ªßi ro cao."
    
    p = doc.add_paragraph(de_xuat, style='Heading 3')
    p.style.font.size = Pt(12)
    p = doc.add_paragraph(f"L√Ω do: {ly_do}")
    p.style.font.size = Pt(11)
    doc.add_paragraph()
    
    # Ch·ªØ k√Ω
    p = doc.add_paragraph('_' * 50)
    p.alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph()
    
    table_sign = doc.add_table(rows=2, cols=2)
    table_sign.style = 'Table Grid'
    table_sign.alignment = WD_TABLE_ALIGNMENT.CENTER
    table_sign.autofit = True
    for row in table_sign.rows:
        for cell in row.cells:
            cell.paragraphs[0].style.font.size = Pt(11)
            cell.paragraphs[0].style.font.name = 'Times New Roman'
    
    cells = table_sign.rows[0].cells
    cells[0].text = 'Ng∆∞·ªùi th·∫©m ƒë·ªãnh'
    cells[1].text = 'Ph√™ duy·ªát'
    
    cells = table_sign.rows[1].cells
    cells[0].text = '(K√Ω, ghi r√µ h·ªç t√™n)'
    cells[1].text = '(K√Ω, ghi r√µ h·ªç t√™n)'
    
    buffer = io.BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer.read()
def export_to_pdf(data: Dict[str, Any], metrics: Dict[str, Any], schedule_df: pd.DataFrame, analysis: str = "") -> str:
    """Xu·∫•t b√°o c√°o th·∫©m ƒë·ªãnh ra LaTeX ƒë·ªÉ t·∫°o PDF"""
    latex_content = r"""
\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{vietnam}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{booktabs}
\usepackage{array}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{setspace}
\usepackage[sc]{mathpazo}
\usepackage{noto}
\renewcommand{\familydefault}{\rmdefault}
\begin{document}

\begin{titlepage}
    \centering
    \vspace*{1cm}
    {\LARGE \textbf{NG√ÇN H√ÄNG N√îNG NGHI·ªÜP V√Ä PH√ÅT TRI·ªÇN N√îNG TH√îN VI·ªÜT NAM}\par}
    \vspace{1cm}
    {\Huge \textbf{B√ÅO C√ÅO TH·∫®M ƒê·ªäNH PH∆Ø∆†NG √ÅN S·ª¨ D·ª§NG V·ªêN}\par}
    \vspace{2cm}
    {\large Ng√†y b√°o c√°o: """ + dt.date.today().strftime("%d/%m/%Y") + r"""\par}
    \vspace{2cm}
    {\large \textit{H·ªì s∆° kh√°ch h√†ng: """ + data.get('ten_khach_hang', 'Kh√¥ng x√°c ƒë·ªãnh') + r"""}\par}
\end{titlepage}

\section*{I. Th√¥ng tin kh√°ch h√†ng}
\begin{tabular}{|>{\raggedright\arraybackslash}p{5cm}|>{\raggedright\arraybackslash}p{10cm}|}
    \hline
    \textbf{Ch·ªâ ti√™u} & \textbf{Gi√° tr·ªã} \\ \hline
    H·ªç v√† t√™n & """ + data.get('ten_khach_hang', '') + r""" \\ \hline
    CMND/CCCD & """ + data.get('cccd', '') + r""" \\ \hline
    N∆°i c∆∞ tr√∫ & """ + data.get('noi_cu_tru', '') + r""" \\ \hline
    S·ªë ƒëi·ªán tho·∫°i & """ + data.get('so_dien_thoai', '') + r""" \\ \hline
    M·ª•c ƒë√≠ch vay & """ + data.get('muc_dich_vay', '') + r""" \\ \hline
\end{tabular}

\vspace{1cm}

\section*{II. Th√¥ng tin kho·∫£n vay}
\begin{tabular}{|>{\raggedright\arraybackslash}p{5cm}|>{\raggedright\arraybackslash}p{10cm}|}
    \hline
    \textbf{Ch·ªâ ti√™u} & \textbf{Gi√° tr·ªã} \\ \hline
    T·ªïng nhu c·∫ßu v·ªën & """ + format_vnd(data.get('tong_nhu_cau_von', 0)) + r""" VND \\ \hline
    V·ªën ƒë·ªëi ·ª©ng & """ + format_vnd(data.get('von_doi_ung', 0)) + r""" VND \\ \hline
    S·ªë ti·ªÅn vay & """ + format_vnd(data.get('so_tien_vay', 0)) + r""" VND \\ \hline
    L√£i su·∫•t & """ + f"{data.get('lai_suat_nam', 0):.2f}" + r"""\%/nƒÉm \\ \hline
    Th·ªùi h·∫°n vay & """ + f"{data.get('thoi_gian_vay_thang', 0)}" + r""" th√°ng \\ \hline
    Thu nh·∫≠p th√°ng & """ + format_vnd(data.get('thu_nhap_thang', 0)) + r""" VND \\ \hline
    Gi√° tr·ªã TSƒêB & """ + format_vnd(data.get('gia_tri_tsdb', 0)) + r""" VND \\ \hline
\end{tabular}

\vspace{1cm}

\section*{III. Ch·ªâ ti√™u t√†i ch√≠nh (CADAP)}
\begin{tabular}{|>{\raggedright\arraybackslash}p{5cm}|>{\raggedright\arraybackslash}p{5cm}|>{\raggedright\arraybackslash}p{5cm}|}
    \hline
    \textbf{Ch·ªâ ti√™u} & \textbf{Gi√° tr·ªã} & \textbf{ƒê√°nh gi√°} \\ \hline
    PMT (Ti·ªÅn tr·∫£/th√°ng) & """ + format_vnd(metrics.get('PMT_thang', 0)) + r""" VND & \\ \hline
    DSR (Debt Service Ratio) & """ + (f"{metrics.get('DSR', 0)*100:.1f}\%" if not np.isnan(metrics.get('DSR', 0)) else 'n/a') + r""" & """ + ('‚úì ƒê·∫°t' if (not np.isnan(metrics.get('DSR', 0)) and metrics.get('DSR', 0) <= 0.8) else '‚úó Kh√¥ng ƒë·∫°t') + r""" \\ \hline
    LTV (Loan to Value) & """ + (f"{metrics.get('LTV', 0)*100:.1f}\%" if not np.isnan(metrics.get('LTV', 0)) else 'n/a') + r""" & """ + ('‚úì ƒê·∫°t' if (not np.isnan(metrics.get('LTV', 0)) and metrics.get('LTV', 0) <= 0.8) else '‚úó Kh√¥ng ƒë·∫°t') + r""" \\ \hline
    E/C (Equity to Capital) & """ + (f"{metrics.get('E_over_C', 0)*100:.1f}\%" if not np.isnan(metrics.get('E_over_C', 0)) else 'n/a') + r""" & """ + ('‚úì ƒê·∫°t' if (not np.isnan(metrics.get('E_over_C', 0)) and metrics.get('E_over_C', 0) >= 0.2) else '‚úó Kh√¥ng ƒë·∫°t') + r""" \\ \hline
    CFR (Cash Flow Ratio) & """ + (f"{metrics.get('CFR', 0)*100:.1f}\%" if not np.isnan(metrics.get('CFR', 0)) else 'n/a') + r""" & """ + ('‚úì ƒê·∫°t' if (not np.isnan(metrics.get('CFR', 0)) and metrics.get('CFR', 0) > 0) else '‚úó Kh√¥ng ƒë·∫°t') + r""" \\ \hline
    Coverage (Collateral Coverage) & """ + (f"{metrics.get('Coverage', 0)*100:.1f}\%" if not np.isnan(metrics.get('Coverage', 0)) else 'n/a') + r""" & """ + ('‚úì ƒê·∫°t' if (not np.isnan(metrics.get('Coverage', 0)) and metrics.get('Coverage', 0) > 1.2) else '‚úó Kh√¥ng ƒë·∫°t') + r""" \\ \hline
    Score t·ªïng h·ª£p & """ + f"{metrics.get('Score_AI_demo', 0):.3f}" + r""" & """ + ('‚úì T·ªët' if metrics.get('Score_AI_demo', 0) >= 0.7 else ('‚ö† Trung b√¨nh' if metrics.get('Score_AI_demo', 0) >= 0.5 else '‚úó Y·∫øu')) + r""" \\ \hline
\end{tabular}

\vspace{1cm}

\section*{IV. K·∫ø ho·∫°ch tr·∫£ n·ª£ (5 k·ª≥ ƒë·∫ßu)}
\begin{tabular}{|c|c|r|r|r|r|}
    \hline
    \textbf{K·ª≥} & \textbf{Ng√†y} & \textbf{Ti·ªÅn l√£i} & \textbf{Ti·ªÅn g·ªëc} & \textbf{T·ªïng tr·∫£} & \textbf{D∆∞ n·ª£} \\ \hline
"""
    for i in range(min(5, len(schedule_df))):
        row = schedule_df.iloc[i]
        latex_content += f"    {row['K·ª≥']} & {row['Ng√†y thanh to√°n']} & {format_vnd(row['Ti·ªÅn l√£i'])} & {format_vnd(row['Ti·ªÅn g·ªëc'])} & {format_vnd(row['T·ªïng ph·∫£i tr·∫£'])} & {format_vnd(row['D∆∞ n·ª£ c√≤n l·∫°i'])} \\\\ \\hline\n"
    
    latex_content += r"""
\end{tabular}

\begin{center}
(Xem file Excel ƒë√≠nh k√®m ƒë·ªÉ c√≥ ƒë·∫ßy ƒë·ªß """ + str(len(schedule_df)) + r""" k·ª≥ thanh to√°n)
\end{center}

\vspace{1cm}
"""
    if analysis:
        latex_content += r"""
\section*{V. Ph√¢n t√≠ch v√† K·∫øt lu·∫≠n (AI)}
""" + analysis.replace('\n', '\\\\\n') + r"""

\vspace{1cm}
"""
    
    # √ù ki·∫øn th·∫©m ƒë·ªãnh
    score = metrics.get('Score_AI_demo', 0)
    dsr = metrics.get('DSR', 0)
    ltv = metrics.get('LTV', 0)
    
    if score >= 0.7 and (np.isnan(dsr) or dsr <= 0.8) and (np.isnan(ltv) or ltv <= 0.8):
        de_xuat = "‚òë ƒê·ªÄ XU·∫§T CHO VAY"
        ly_do = "H·ªì s∆° ƒë√°p ·ª©ng c√°c ch·ªâ ti√™u t√†i ch√≠nh, kh·∫£ nƒÉng tr·∫£ n·ª£ t·ªët, t√†i s·∫£n b·∫£o ƒë·∫£m ƒë·∫ßy ƒë·ªß."
    elif score >= 0.5:
        de_xuat = "‚òë ƒê·ªÄ XU·∫§T CHO VAY C√ì ƒêI·ªÄU KI·ªÜN"
        ly_do = "H·ªì s∆° c·∫ßn b·ªï sung th√™m t√†i s·∫£n b·∫£o ƒë·∫£m ho·∫∑c ƒëi·ªÅu ch·ªânh ƒëi·ªÅu ki·ªán vay ƒë·ªÉ gi·∫£m r·ªßi ro."
    else:
        de_xuat = "‚òê KH√îNG ƒê·ªÄ XU·∫§T CHO VAY"
        ly_do = "H·ªì s∆° kh√¥ng ƒë·∫°t c√°c ch·ªâ ti√™u t√†i ch√≠nh t·ªëi thi·ªÉu, r·ªßi ro cao."
    
    latex_content += r"""
\section*{VI. √ù ki·∫øn th·∫©m ƒë·ªãnh}
\textbf{""" + de_xuat + r"""} \\
L√Ω do: """ + ly_do + r"""

\vspace{1cm}

\begin{center}
\rule{5cm}{0.4pt}
\end{center}

\begin{tabular}{p{7cm}p{7cm}}
Ng∆∞·ªùi th·∫©m ƒë·ªãnh & Ph√™ duy·ªát \\
(K√Ω, ghi r√µ h·ªç t√™n) & (K√Ω, ghi r√µ h·ªç t√™n) \\
\end{tabular}

\end{document}
"""
    return latex_content
# ========================== UI ==========================
st.title("üíº Th·∫©m ƒë·ªãnh ph∆∞∆°ng √°n s·ª≠ d·ª•ng v·ªën (PASDV)")
st.caption("Upload .docx ‚Üí Tr√≠ch xu·∫•t ‚Üí Ch·ªânh s·ª≠a ‚Üí T√≠nh ch·ªâ ti√™u ‚Üí K·∫ø ho·∫°ch tr·∫£ n·ª£ ‚Üí Ph√¢n t√≠ch AI ‚Üí Xu·∫•t Excel/ZIP")
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh & Gemini")
    model_name = st.selectbox("Model Gemini", ["gemini-2.0-flash-exp", "gemini-1.5-pro", "gemini-1.5-flash"], index=0)
    api_key = st.text_input("API Key Gemini", type="password", help="Ho·∫∑c set GENAI_API_KEY trong secrets.")
    if not api_key:
        api_key = st.secrets.get("GENAI_API_KEY", "") if hasattr(st, "secrets") else ""
    st.markdown("---")

uploaded = st.file_uploader("T·∫£i l√™n h·ªì s∆° ph∆∞∆°ng √°n pasdv.docx", type=["docx"], help="Ch·ªâ c·∫ßn m·ªôt file .docx")
data = FIELD_DEFAULTS.copy()
if uploaded is not None:
    try:
        data.update(extract_from_docx(uploaded.read()))
        st.success("‚úÖ ƒê√£ tr√≠ch xu·∫•t s∆° b·ªô t·ª´ file.")
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file DOCX: {e}")
st.markdown("""
<style>
.info-box {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    padding: 20px;
    border-radius: 10px;
    margin-bottom: 20px;
    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
}
.info-box h3 {
    color: white;
    margin: 0;
}
</style>
<div class="info-box">
    <h3>üìã 1) Th√¥ng tin kh√°ch h√†ng & kho·∫£n vay</h3>
</div>
""", unsafe_allow_html=True)
col1, col2, col3 = st.columns(3)
with col1:
    data["ten_khach_hang"] = st.text_input("H·ªç t√™n KH", value=data["ten_khach_hang"])
    data["cccd"] = st.text_input("CCCD/CMND", value=data["cccd"])
    data["noi_cu_tru"] = st.text_input("N∆°i c∆∞ tr√∫", value=data["noi_cu_tru"])
    data["so_dien_thoai"] = st.text_input("S·ªë ƒëi·ªán tho·∫°i", value=data["so_dien_thoai"])
with col2:
    data["muc_dich_vay"] = st.text_input("M·ª•c ƒë√≠ch vay", value=data["muc_dich_vay"])
    data["tong_nhu_cau_von"] = vn_money_input("T·ªïng nhu c·∫ßu v·ªën (VND)", data["tong_nhu_cau_von"])
    data["von_doi_ung"] = vn_money_input("V·ªën ƒë·ªëi ·ª©ng (VND)", data["von_doi_ung"])
    data["so_tien_vay"] = vn_money_input("S·ªë ti·ªÅn vay (VND)", data["so_tien_vay"])
with col3:
    data["lai_suat_nam"] = vn_percent_input("L√£i su·∫•t (%/nƒÉm)", data["lai_suat_nam"])
    data["thoi_gian_vay_thang"] = st.number_input("Th·ªùi gian vay (th√°ng)", value=int(data["thoi_gian_vay_thang"]), min_value=1, max_value=480, step=1)
    data["thu_nhap_thang"] = vn_money_input("Thu nh·∫≠p th√°ng (VND)", data["thu_nhap_thang"])
    data["gia_tri_tsdb"] = vn_money_input("Gi√° tr·ªã TSƒêB (VND)", data["gia_tri_tsdb"])
col4, col5 = st.columns(2)
with col4:
    data["tong_no_hien_tai"] = vn_money_input("T·ªïng n·ª£ hi·ªán t·∫°i (VND)", data["tong_no_hien_tai"])
with col5:
    data["tong_von_dau_tu"] = vn_money_input("T·ªïng v·ªën ƒë·∫ßu t∆∞ (VND)", data["tong_von_dau_tu"])
    data["loi_nhuan_rong_nam"] = vn_money_input("L·ª£i nhu·∫≠n r√≤ng nƒÉm (VND)", data["loi_nhuan_rong_nam"])
st.markdown("---")
st.subheader("2) Ch·ªâ ti√™u t√†i ch√≠nh (CADAP)")
metrics = compute_metrics(data)
if go is not None:
    create_metrics_chart(metrics)
else:
    st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì. Vui l√≤ng c√†i ƒë·∫∑t th∆∞ vi·ªán Plotly.")
mcol1, mcol2, mcol3, mcol4 = st.columns(4)
with mcol1:
    st.metric("PMT (VND/th√°ng)", f"{format_vnd(metrics['PMT_thang'])}")
    st.metric("DSR (‚â§80%)", f"{metrics['DSR']*100:,.1f}%" if not np.isnan(metrics["DSR"]) else "n/a")
with mcol2:
    st.metric("LTV (‚â§80%)", f"{metrics['LTV']*100:,.1f}%" if not np.isnan(metrics["LTV"]) else "n/a")
    st.metric("E/C (‚â•20%)", f"{metrics['E_over_C']*100:,.1f}%" if not np.isnan(metrics["E_over_C"]) else "n/a")
with mcol3:
    st.metric("Debt/Income (<4)", f"{metrics['Debt_over_Income']:,.2f}" if not np.isnan(metrics["Debt_over_Income"]) else "n/a")
    st.metric("CFR (>0)", f"{metrics['CFR']*100:,.1f}%" if not np.isnan(metrics["CFR"]) else "n/a")
with mcol4:
    st.metric("Coverage (>120%)", f"{metrics['Coverage']*100:,.1f}%" if not np.isnan(metrics["Coverage"]) else "n/a")
    st.metric("Score demo", f"{metrics['Score_AI_demo']:,.3f}")
ok_flag = "‚úÖ" if metrics["Phuong_an_hop_ly"] else "‚ö†Ô∏è"
st.info(f"{ok_flag} T·ªïng nhu c·∫ßu v·ªën {'=' if metrics['Phuong_an_hop_ly'] else '‚â†'} v·ªën ƒë·ªëi ·ª©ng + s·ªë ti·ªÅn vay")
st.markdown("---")
st.markdown("""
<div class="info-box">
    <h3>üí∞ 3) K·∫ø ho·∫°ch tr·∫£ n·ª£</h3>
</div>
""", unsafe_allow_html=True)
schedule_df = build_amortization(
    principal=data["so_tien_vay"],
    annual_rate_pct=data["lai_suat_nam"],
    months=int(data["thoi_gian_vay_thang"]),
    start_date=dt.date.today()
)
styled_table = style_schedule_table(schedule_df)
st.dataframe(styled_table, use_container_width=True, height=400)
out = io.BytesIO()
with pd.ExcelWriter(out, engine="openpyxl") as writer:
    df_data = pd.DataFrame([data])
    for col in ['tong_nhu_cau_von', 'von_doi_ung', 'so_tien_vay', 'thu_nhap_thang',
                'gia_tri_tsdb', 'tong_no_hien_tai', 'loi_nhuan_rong_nam', 'tong_von_dau_tu']:
        if col in df_data.columns:
            df_data[col] = df_data[col].apply(lambda x: format_vnd(x) if x is not None else None)
    df_metrics = pd.DataFrame([metrics])
    for col in ['PMT_thang']:
        if col in df_metrics.columns:
            df_metrics[col] = df_metrics[col].apply(lambda x: format_vnd(x) if x is not None else None)
    for col in ['DSR', 'LTV', 'E_over_C', 'CFR', 'Coverage', 'ROI']:
        if col in df_metrics.columns:
            df_metrics[col] = df_metrics[col].apply(lambda x: f"{x*100:,.2f}%" if not np.isnan(x) else 'n/a')
    df_data.to_excel(writer, sheet_name="Thong_tin", index=False)
    df_metrics.to_excel(writer, sheet_name="Chi_tieu", index=False)
    schedule_df.to_excel(writer, sheet_name="Ke_hoach", index=False)
out.seek(0)
st.download_button("‚¨áÔ∏è T·∫£i Excel", data=out, file_name="ke_hoach_tra_no.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
# Initialize analysis variable to avoid NameError
analysis = ""
st.subheader("4) Ph√¢n t√≠ch AI (Gemini)")
if api_key and genai is not None:
    with st.spinner("ƒêang ph√¢n t√≠ch..."):
        analysis = gemini_analyze(data, metrics, model_name=model_name, api_key=api_key)
    st.markdown("**K·∫øt lu·∫≠n:**")
    st.write(analysis)
else:
    st.warning("Ch∆∞a c√≥ API key Gemini. ƒêi·ªÅn API key ·ªü Sidebar ƒë·ªÉ d√πng t√≠nh nƒÉng n√†y.")
    analysis = ""
# Export Report Section
st.markdown("---")
st.subheader("üìÑ Xu·∫•t B√°o c√°o")
export_format = st.selectbox("Ch·ªçn ƒë·ªãnh d·∫°ng b√°o c√°o", ["DOCX", "PDF"], index=0)
if export_format == "DOCX" and Document is None:
    st.info("üìÑ C√†i ƒë·∫∑t python-docx ƒë·ªÉ xu·∫•t b√°o c√°o DOCX")
elif export_format == "PDF":
    st.info("üìÑ B√°o c√°o PDF s·∫Ω ƒë∆∞·ª£c t·∫°o b·∫±ng LaTeX (y√™u c·∫ßu texlive-full v√† texlive-fonts-extra)")
if st.button("‚¨áÔ∏è T·∫£i B√°o c√°o"):
    file_name = f"bao_cao_tham_dinh_{data.get('ten_khach_hang', 'khach_hang').replace(' ', '_')}_{dt.date.today().strftime('%Y%m%d')}"
    if export_format == "DOCX" and Document is not None:
        docx_buffer = export_to_docx(data, metrics, schedule_df, analysis=analysis)
        st.download_button(
            "T·∫£i B√°o c√°o DOCX",
            data=docx_buffer,
            file_name=f"{file_name}.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
    elif export_format == "PDF":
        try:
            pdf_content = export_to_pdf(data, metrics, schedule_df, analysis=analysis)
            st.download_button(
                "T·∫£i B√°o c√°o PDF",
                data=pdf_content.encode('utf-8'),
                file_name=f"{file_name}.tex",
                mime="text/plain"
            )
            st.info("L∆∞u √Ω: File .tex c·∫ßn ƒë∆∞·ª£c bi√™n d·ªãch b·∫±ng latexmk v·ªõi PDFLaTeX ƒë·ªÉ t·∫°o PDF.")
        except Exception as e:
            st.error(f"L·ªói khi t·∫°o LaTeX: {e}")
    else:
        st.error("Kh√¥ng th·ªÉ xu·∫•t b√°o c√°o do thi·∫øu th∆∞ vi·ªán python-docx.")
st.subheader("5) üí¨ Tr√≤ chuy·ªán v·ªõi AI v·ªÅ h·ªì s∆°")
if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []
for msg in st.session_state.chat_messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
if prompt := st.chat_input("H·ªèi AI v·ªÅ h·ªì s∆° n√†y... (VD: ƒê√°nh gi√° kh·∫£ nƒÉng tr·∫£ n·ª£? R·ªßi ro n√†o c·∫ßn l∆∞u √Ω?)"):
    st.session_state.chat_messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    with st.chat_message("assistant"):
        if not api_key:
            response = "‚ö†Ô∏è Vui l√≤ng nh·∫≠p API Key Gemini ·ªü Sidebar ƒë·ªÉ s·ª≠ d·ª•ng chatbox."
            st.warning(response)
        elif genai is None:
            response = "‚ö†Ô∏è Th∆∞ vi·ªán google-generativeai ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t."
            st.error(response)
        else:
            try:
                with st.spinner("ü§î AI ƒëang suy nghƒ©..."):
                    genai.configure(api_key=api_key)
                    model = genai.GenerativeModel(model_name)
                    context = f"""
B·∫°n l√† chuy√™n vi√™n t√≠n d·ª•ng chuy√™n nghi·ªáp. D∆∞·ªõi ƒë√¢y l√† th√¥ng tin h·ªì s∆° vay:
**Th√¥ng tin kh√°ch h√†ng:**
- H·ªç t√™n: {data['ten_khach_hang']}
- CCCD: {data['cccd']}
- ƒê·ªãa ch·ªâ: {data['noi_cu_tru']}
- SƒêT: {data['so_dien_thoai']}
**Ph∆∞∆°ng √°n vay:**
- M·ª•c ƒë√≠ch: {data['muc_dich_vay']}
- T·ªïng nhu c·∫ßu v·ªën: {format_vnd(data['tong_nhu_cau_von'])} VND
- V·ªën ƒë·ªëi ·ª©ng: {format_vnd(data['von_doi_ung'])} VND
- S·ªë ti·ªÅn vay: {format_vnd(data['so_tien_vay'])} VND
- L√£i su·∫•t: {data['lai_suat_nam']}%/nƒÉm
- Th·ªùi h·∫°n: {data['thoi_gian_vay_thang']} th√°ng
- Thu nh·∫≠p th√°ng: {format_vnd(data['thu_nhap_thang'])} VND
- Gi√° tr·ªã TSƒêB: {format_vnd(data['gia_tri_tsdb'])} VND
**Ch·ªâ ti√™u t√†i ch√≠nh:**
- PMT (ti·ªÅn tr·∫£ h√†ng th√°ng): {format_vnd(metrics['PMT_thang'])} VND
- DSR: {metrics['DSR']*100:.1f}% (chu·∫©n ‚â§80%)
- LTV: {metrics['LTV']*100:.1f}% (chu·∫©n ‚â§80%)
- E/C: {metrics['E_over_C']*100:.1f}% (chu·∫©n ‚â•20%)
- CFR: {metrics['CFR']*100:.1f}% (chu·∫©n >0%)
- Coverage: {metrics['Coverage']*100:.1f}% (chu·∫©n >120%)
- Score t·ªïng h·ª£p: {metrics['Score_AI_demo']:.3f}
H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau d·ª±a tr√™n th√¥ng tin tr√™n, s·ª≠ d·ª•ng ti·∫øng Vi·ªát chuy√™n nghi·ªáp nh∆∞ng d·ªÖ hi·ªÉu:
"""
                    full_prompt = context + "\n\nC√¢u h·ªèi: " + prompt
                    resp = model.generate_content(full_prompt)
                    response = resp.text if resp.text else "‚ö†Ô∏è Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI."
                    st.markdown(response)
            except Exception as e:
                response = f"‚ùå L·ªói khi g·ªçi Gemini: {str(e)}"
                st.error(response)
        st.session_state.chat_messages.append({"role": "assistant", "content": response})
col_clear, col_export = st.columns([1, 3])
with col_clear:
    if st.button("üóëÔ∏è X√≥a chat"):
        st.session_state.chat_messages = []
        st.rerun()
